# calhack23_winning_project

# 🤖 The GitHub for LLM Hallucinations 🤖 

## Outline
*  💡 Motivation
*  🛠 The Platform
*  ⚙️ The Features 
*  👫 Contributors 
*  👉 Call To Action

## 💡 Motivation
 OpenAI recently released their $1M grant program for increasing democratic AI.🤑 Language models have the power 🔋to control our world's discourse. 🗣We need to ensure they are properly monitored 👀 and maintained for bias, hallucinations, and more. 🧠   

## 🛠 The Platform
The GitHub for LLM Hallucinations. 😱Just like any coder today can find open source code, contribute, and report issues with a repository, any person in the world 🌎can use and report hallucinations and bias they've observed, contributing to the platform.🚀 This alerts the model developers, 👨‍💻gives the developers actionable data to run RLHF to fix the issue, creates transparency within the public, 😎 and enables a common platform for community discourse over how these models should behave in certain grey areas. ⬛️  

## ⚙️ The Features   

 \* Verified users only can create issues: Only users who have verified their identity can submit new issues reporting hallucinations or bias. This prevents spam and ensures high quality data. 
 
 \* Upvote/downvote issues you agree/disagree with 👍👎: Users can upvote or downvote existing issues to signal their agreement or disagreement. The issues with the most upvotes will gain the most attention from developers.
 
 \* Make comments 💬: Users can comment on issues to provide more context, examples, or opinions. The discussions and comments enrich the issue reporting process.  

 \* Vote on preferential responses so those models can learn🤓: For some issues, model developers may propose different responses or fixes. Users can vote on which response they prefer so the models can be retrained accordingly.  

 \* Support for GPT-4, GPT-3.5, Claude, Llama, Falcon, etc. 🦅: The platform supports reporting issues for many different language models including GPT-3 variants, Claude, Llama, and Falcon. The issues are tagged with the model so developers know which model needs retraining.

  
https://devpost.com/software/aligned-the-github-for-llm-hallucinations
