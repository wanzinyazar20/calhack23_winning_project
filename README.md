# calhack23_winning_project

# ğŸ¤– The GitHub for LLM Hallucinations ğŸ¤– 

## Outline
*  ğŸ’¡ Motivation
*  ğŸ›  The Platform
*  âš™ï¸ The Features 
*  ğŸ‘« Contributors 
*  ğŸ‘‰ Call To Action

## ğŸ’¡ Motivation
 OpenAI recently released their $1M grant program for increasing democratic AI.ğŸ¤‘ Language models have the power ğŸ”‹to control our world's discourse. ğŸ—£We need to ensure they are properly monitored ğŸ‘€ and maintained for bias, hallucinations, and more. ğŸ§    

## ğŸ›  The Platform
The GitHub for LLM Hallucinations. ğŸ˜±Just like any coder today can find open source code, contribute, and report issues with a repository, any person in the world ğŸŒcan use and report hallucinations and bias they've observed, contributing to the platform.ğŸš€ This alerts the model developers, ğŸ‘¨â€ğŸ’»gives the developers actionable data to run RLHF to fix the issue, creates transparency within the public, ğŸ˜ and enables a common platform for community discourse over how these models should behave in certain grey areas. â¬›ï¸  

## âš™ï¸ The Features   

 \* Verified users only can create issues: Only users who have verified their identity can submit new issues reporting hallucinations or bias. This prevents spam and ensures high quality data. 
 
 \* Upvote/downvote issues you agree/disagree with ğŸ‘ğŸ‘: Users can upvote or downvote existing issues to signal their agreement or disagreement. The issues with the most upvotes will gain the most attention from developers.
 
 \* Make comments ğŸ’¬: Users can comment on issues to provide more context, examples, or opinions. The discussions and comments enrich the issue reporting process.  

 \* Vote on preferential responses so those models can learnğŸ¤“: For some issues, model developers may propose different responses or fixes. Users can vote on which response they prefer so the models can be retrained accordingly.  

 \* Support for GPT-4, GPT-3.5, Claude, Llama, Falcon, etc. ğŸ¦…: The platform supports reporting issues for many different language models including GPT-3 variants, Claude, Llama, and Falcon. The issues are tagged with the model so developers know which model needs retraining.

  
https://devpost.com/software/aligned-the-github-for-llm-hallucinations
